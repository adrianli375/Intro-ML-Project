{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d38cc43-12c5-48b8-9399-526cdf24e947",
   "metadata": {},
   "source": [
    "# Demo 3: A classification task involving Natural Langauge Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22104f-1061-41ee-aefc-983b8f52e859",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92547b7e-47f7-4f9a-a5a9-4651da55c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dde04-52dd-461b-9bbe-9be9eb86509a",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210aff76-3d94-44f8-903d-7298820fc7aa",
   "metadata": {},
   "source": [
    "This dataset is taken from [Twitter Financial News dataset](https://www.kaggle.com/datasets/sulphatet/twitter-financial-news) from Kaggle. We are given text as input and we need to predict the corresponding category of the financial news. \n",
    "\n",
    "According to the documentation, the categories are listed as below: \n",
    "\n",
    "    \"LABEL_0\": \"Analyst Update\",\n",
    "\n",
    "    \"LABEL_1\": \"Fed | Central Banks\",\n",
    "\n",
    "    \"LABEL_2\": \"Company | Product News\",\n",
    "\n",
    "    \"LABEL_3\": \"Treasuries | Corporate Debt\",\n",
    "\n",
    "    \"LABEL_4\": \"Dividend\",\n",
    "\n",
    "    \"LABEL_5\": \"Earnings\",\n",
    "\n",
    "    \"LABEL_6\": \"Energy | Oil\",\n",
    "\n",
    "    \"LABEL_7\": \"Financials\",\n",
    "\n",
    "    \"LABEL_8\": \"Currencies\",\n",
    "\n",
    "    \"LABEL_9\": \"General News | Opinion\",\n",
    "\n",
    "    \"LABEL_10\": \"Gold | Metals | Materials\",\n",
    "\n",
    "    \"LABEL_11\": \"IPO\",\n",
    "\n",
    "    \"LABEL_12\": \"Legal | Regulation\",\n",
    "\n",
    "    \"LABEL_13\": \"M&A | Investments\",\n",
    "\n",
    "    \"LABEL_14\": \"Macro\",\n",
    "\n",
    "    \"LABEL_15\": \"Markets\",\n",
    "\n",
    "    \"LABEL_16\": \"Politics\",\n",
    "\n",
    "    \"LABEL_17\": \"Personnel Change\",\n",
    "\n",
    "    \"LABEL_18\": \"Stock Commentary\",\n",
    "\n",
    "    \"LABEL_19\": \"Stock Movement\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a3479-9d53-49a6-ade7-52938bf9e04c",
   "metadata": {},
   "source": [
    "There are two possible ways to handle this data and build a predictive model. In the previous notebook (Part A), we focused on using Bag of Words representation. In this notebook (Part B), we will focus on using some pre-trained NLP libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f5d8d-c0c6-4804-a251-c819c909331e",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97bc0f-3dca-4f72-b349-630b3bdd2f4d",
   "metadata": {},
   "source": [
    "First, we read in the training and validation data provided. We will further split the training set into training and validation data. The validation data provided will be used as the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636c86f2-e41c-4667-8e0d-ba764c23c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../datasets/train_data.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/valid_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2b0e7-e758-42b6-9b04-8badd1a19a51",
   "metadata": {},
   "source": [
    "Let's have a look of how the training data looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd80282-17dc-4999-8b12-7b7560ff648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16985</th>\n",
       "      <td>KfW credit line for Uniper could be raised to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16986</th>\n",
       "      <td>KfW credit line for Uniper could be raised to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16987</th>\n",
       "      <td>Russian  https://t.co/R0iPhyo5p7 sells 1 bln r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16988</th>\n",
       "      <td>Global ESG bond issuance posts H1 dip as supra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16989</th>\n",
       "      <td>Brazil's Petrobras says it signed a $1.25 bill...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Here are Thursday's biggest analyst calls: App...      0\n",
       "1      Buy Las Vegas Sands as travel to Singapore bui...      0\n",
       "2      Piper Sandler downgrades DocuSign to sell, cit...      0\n",
       "3      Analysts react to Tesla's latest earnings, bre...      0\n",
       "4      Netflix and its peers are set for a ‘return to...      0\n",
       "...                                                  ...    ...\n",
       "16985  KfW credit line for Uniper could be raised to ...      3\n",
       "16986  KfW credit line for Uniper could be raised to ...      3\n",
       "16987  Russian  https://t.co/R0iPhyo5p7 sells 1 bln r...      3\n",
       "16988  Global ESG bond issuance posts H1 dip as supra...      3\n",
       "16989  Brazil's Petrobras says it signed a $1.25 bill...      3\n",
       "\n",
       "[16990 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248f6e3-bfbe-4dfb-8130-5187c0bbd38a",
   "metadata": {},
   "source": [
    "Let's split our data into `X` and `y`, with their own training, validation and test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a670ba1-4e3e-4409-9120-2552770e9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df[\"text\"], train_df[\"label\"]\n",
    "X_test, y_test = test_df[\"text\"], test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d4c59-3b1c-4af9-bddc-5eb1e0fb9fe4",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f500b96-f357-4075-8dc5-771df96f215b",
   "metadata": {},
   "source": [
    "At first glance, there are some hyperlinks present in the text which may contaminate our data and affect the performance. We define a function to remove the hyperlinks from the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a32a85-9fcb-4b95-a57f-1dc975ee3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(text_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    url_pattern = re.compile(r\"http(s)?://\\S+\")\n",
    "    text_df = text_df.apply(lambda text: re.sub(url_pattern, '', text))\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b149e3-21d2-4f7e-8b30-26c2f81e2318",
   "metadata": {},
   "source": [
    "Let's consider an example with a hyperlink: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef0a75b-4bfe-4639-bf37-8f55867f44e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEE Expands Sustainable Portfolio With Launch of Innovative Paper Bubble Mailer  https://t.co/R1OebY7H6L  https://t.co/b66wE3yvPI'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[926]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad36d2-e3bc-483d-933b-2dc6352a3387",
   "metadata": {},
   "source": [
    "Let's apply our newly defined function `remove_hyperlinks` and see the corresponding output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d956a72-6d4a-4f52-b403-16fcd43042c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEE Expands Sustainable Portfolio With Launch of Innovative Paper Bubble Mailer    '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hyperlinks(X_train).iloc[926]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae4772-776b-4e93-819b-fd119415efa5",
   "metadata": {},
   "source": [
    "## Part B: Using pre-trained NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9c54e-fe0c-4556-9129-c32fef86ac4e",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is an integral part of many modern applications, from chatbots to machine translation and sentiment analysis. With the vast amounts of textual data generated every day, there is an increasing need for tools that can accurately analyze and extract meaningful information from this data. Pre-trained NLP libraries, such as `nltk` and `spacy` are designed for this need. In this demo, we will focus on discussing the usage of spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74f15c-6d2a-439a-8f8b-bfbff5589b17",
   "metadata": {},
   "source": [
    "Spacy is a powerful NLP library that provides a wide range of features, including fast and efficient text processing, advanced linguistic analysis, and support for deep learning models. Spacy is designed to be easy to use, and its pre-trained models enable users to quickly perform a variety of common NLP tasks without the need for extensive training or expertise. By using Spacy, developers and data scientists can save time and resources, while also gaining access to cutting-edge NLP technology that can help to unlock the full potential of their text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd684bf-eb45-4cb9-bac7-a9e59b3f0510",
   "metadata": {},
   "source": [
    "Let's try to explore the training data using `spacy`. First, we load the model before using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e29cca1-0895-4931-8cea-bc0737d22342",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b328f5c-03f6-45cb-8e51-5b4652076287",
   "metadata": {},
   "source": [
    "Recall that we have previously used `TruncatedSVD`, which is a variant of `PCA`, to reduce the dimensionality of the features. Pre-trained libraries like `spacy` also transforms raw sentences into some features with lower dimensions, in which they are called sentence embeddings. The dimension of each embedded sentence is `300`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839748bd-5ff4-4c23-a986-50f7b9b12d9b",
   "metadata": {},
   "source": [
    "Let's extract a random sample from the data and get the sentence embedding of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963fef61-e5da-4bdc-bbb7-75dbd0c3df6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Selected as Delta’s Preferred Cloud Provider  https://t.co/6NDB9DjMzU  https://t.co/33zuRfvkYu\n"
     ]
    }
   ],
   "source": [
    "selected_text = X_train.iloc[2211]\n",
    "print(selected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f931ba9-7413-43dc-b7b5-7a7b97953947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.3421407e-01,  4.0525827e-01, -1.9024168e-01,  1.1192417e-01,\n",
       "        3.4868500e+00,  9.9555415e-01, -1.1839241e+00,  1.6747411e-01,\n",
       "       -1.0778084e+00, -2.0522883e+00,  4.0652585e+00, -6.9855839e-01,\n",
       "       -2.2036667e+00,  3.4582505e-01, -7.1054751e-01,  1.1283466e+00,\n",
       "        1.4741750e+00,  2.1859000e+00, -2.8289163e-01,  1.0027727e-01,\n",
       "        9.7559649e-01, -1.5284873e+00, -9.5997500e-01, -6.3331157e-01,\n",
       "        7.2001912e-02, -1.8117576e+00, -1.2292227e+00, -7.2013330e-01,\n",
       "       -5.2782416e-01, -2.0157583e-01, -1.3222781e+00,  9.4262165e-01,\n",
       "       -1.3730808e+00, -1.0827259e+00, -8.4195834e-01, -9.2275836e-02,\n",
       "        1.9563252e-01,  2.1243913e-01, -8.4608896e-03, -2.0311692e+00,\n",
       "        4.7339168e-01, -5.2695495e-01, -3.4479773e+00, -4.1158918e-01,\n",
       "       -1.5155476e+00,  6.6906500e-01,  2.0562582e+00, -1.9486855e+00,\n",
       "        1.7709298e-01, -1.5825042e+00,  1.4722586e-01,  9.1526359e-01,\n",
       "        2.4090834e-01, -2.0232999e+00,  6.7629136e-02, -2.5324914e-01,\n",
       "       -3.0533745e+00,  6.0553819e-01, -1.7821665e-01, -2.5856502e+00,\n",
       "        1.6809460e+00, -1.8882747e-01,  2.8206646e-02, -4.3519983e-01,\n",
       "        1.0728565e+00,  1.6699916e-01, -1.4898200e+00, -3.1265459e+00,\n",
       "        1.3522752e-01,  1.3121033e+00, -1.0624505e-01, -1.7191925e+00,\n",
       "       -1.8653475e+00,  4.1122892e-01,  4.2072678e-01,  8.7296748e-01,\n",
       "       -7.3276919e-01,  1.0589851e+00, -1.0048583e+00,  1.1969999e+00,\n",
       "       -1.0668815e+00, -9.0339833e-01, -9.3307829e-01,  1.1796716e+00,\n",
       "       -2.2213340e-02,  2.2246248e-01, -1.0510083e+00, -1.4026155e+00,\n",
       "       -5.3019005e-01, -4.6390450e-01,  1.0454010e+00, -8.8876581e-01,\n",
       "        2.1491827e-01, -1.2299833e+00, -1.7244776e+00, -2.0416667e+00,\n",
       "        4.0614748e-01, -1.1957091e+00,  3.3340418e-01,  1.5363343e-01,\n",
       "        8.4234667e-01,  2.2922193e-01, -2.0004399e+00,  2.1230991e+00,\n",
       "        1.1995553e-01,  3.7083085e+00,  1.1730751e+00, -1.2315032e+00,\n",
       "        4.0513352e-02, -1.7795292e+00,  2.0267298e+00,  8.3457893e-01,\n",
       "       -2.2229366e+00,  2.0495823e-02,  3.9486167e-01, -9.5355749e-01,\n",
       "       -1.2504251e+00, -1.5041667e-01, -2.0217650e+00, -8.2070088e-01,\n",
       "       -9.6476001e-01, -1.0719191e+00,  2.4986417e+00,  9.1450459e-01,\n",
       "        9.2036229e-01, -2.7712914e-01, -1.3009671e+00, -2.8061001e+00,\n",
       "        4.4229779e+00, -2.0947571e+00, -2.0367250e+00,  7.0350748e-01,\n",
       "        1.6118418e-01, -1.0342755e-01, -4.8891750e-01, -6.3145167e-01,\n",
       "       -9.5250160e-01,  6.4121324e-01,  1.2347201e+00, -2.1545146e+00,\n",
       "       -1.2684160e-01,  7.2283578e-01,  1.5702850e+00,  5.2711624e-01,\n",
       "       -3.1393167e-01, -1.1461433e+00, -3.1187916e+00,  1.9760509e+00,\n",
       "        3.2494164e-01,  1.7621468e+00,  4.1823229e-01,  5.3501916e-01,\n",
       "       -5.9104126e-02, -3.0600917e-01, -1.2477692e+00, -3.0948088e-01,\n",
       "       -1.0907420e-01,  2.9634764e+00,  2.5060329e-01, -1.0189585e-01,\n",
       "        2.5213234e+00, -8.2684255e-01, -1.2855984e+00,  8.1876254e-01,\n",
       "       -1.6905824e+00, -8.1748837e-01, -2.5227919e+00,  1.0889186e+00,\n",
       "       -2.1216142e+00, -7.4596757e-01, -1.4109083e+00, -2.1302199e+00,\n",
       "        4.2985424e-01,  5.1072663e-01,  6.5458320e-02,  1.7635492e+00,\n",
       "       -2.9368666e-01,  1.6552766e-01,  7.5612515e-02,  6.1463326e-02,\n",
       "       -2.3762231e-01,  3.0270834e+00,  7.7944922e-01,  2.3126251e-01,\n",
       "       -9.0026677e-02,  8.2314318e-01, -1.2799741e+00, -3.9792585e-01,\n",
       "       -2.9446250e-01,  1.1016366e+00, -2.4859196e-02, -1.3180096e+00,\n",
       "        5.3717166e-01, -8.4523994e-01,  3.7060001e-01,  1.1674243e+00,\n",
       "       -1.0542375e+00, -9.8715001e-01,  1.2984458e+00, -2.3467166e+00,\n",
       "       -5.2427584e-01, -1.0585090e-01, -7.7648157e-01,  1.0619000e+00,\n",
       "        2.6165342e+00,  1.7865275e+00, -3.8734510e+00,  1.6100522e+00,\n",
       "        1.3195549e+00,  1.4685999e-01,  7.3323840e-01,  4.9306002e-01,\n",
       "       -1.4769408e+00,  1.7753925e+00,  3.0880335e-01,  1.3464891e+00,\n",
       "        9.2151666e-01, -3.3597498e+00, -3.4399167e-01, -3.8902125e-01,\n",
       "       -1.2532243e+00,  8.6592156e-01, -7.3417884e-01,  1.0745045e+00,\n",
       "       -2.5149834e-01,  1.5673851e+00,  8.1780928e-01,  1.7229752e-01,\n",
       "       -1.1445665e+00, -1.4799590e-01,  1.5307208e+00,  6.9385844e-01,\n",
       "       -6.2168425e-01,  1.4168619e+00, -2.4189909e-01, -6.4560586e-01,\n",
       "       -3.9609149e-03, -7.4903250e-01, -2.9703256e-01,  1.1134425e+00,\n",
       "        1.2913580e-01,  3.0422911e-01,  1.0113691e+00,  2.5699085e-01,\n",
       "        1.6128883e+00,  2.3987749e+00, -2.1306112e+00, -5.3844666e-01,\n",
       "       -2.7584502e-01,  1.2476652e-01,  1.0864924e+00, -1.3949809e+00,\n",
       "       -1.0959100e+00, -1.1997616e+00,  1.3328914e-01, -5.4200839e-02,\n",
       "        8.2743835e-01, -8.2939990e-02,  8.5116166e-01,  8.0758411e-01,\n",
       "       -2.5836756e+00,  5.9055072e-01,  1.1488584e+00,  1.0719942e+00,\n",
       "        2.6819417e-01,  1.1870525e+00,  1.0989016e+00,  4.2328295e-01,\n",
       "       -1.6424541e+00, -1.0261967e+00,  1.2602285e+00,  6.3050836e-02,\n",
       "        6.4453512e-01, -2.1409292e-03,  1.5689276e+00, -2.3074234e+00,\n",
       "       -3.4321249e-01, -9.6560009e-02, -5.2962512e-02,  1.6458501e+00,\n",
       "        2.4080832e+00, -4.2384818e-01, -6.1396331e-01, -1.4260416e+00,\n",
       "        1.0546100e+00, -4.1317996e-01,  1.6292983e+00,  7.8178167e-01,\n",
       "        5.4730630e-01,  6.5307504e-01, -1.8196467e+00, -1.2903270e+00,\n",
       "       -1.6331717e+00,  1.6824101e+00,  6.9636410e-01,  1.5143360e+00,\n",
       "       -1.2761294e+00, -1.8968468e+00,  3.8126688e-02,  1.2291549e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(selected_text)\n",
    "\n",
    "sentence_embedding = doc.vector\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba12818-637f-43b4-9734-cf7bd83f81a8",
   "metadata": {},
   "source": [
    "This is a sentence embedding vector which stores 300 different numerical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0e46c-5661-498a-b0b7-6e852cc30fe6",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3bbd0-f760-4434-b40c-dc29bc6b75b0",
   "metadata": {},
   "source": [
    "Recall from the previous notebook that `RandomForestClassifier` is the best-performing model (although only with a test f1 of `0.70`). Let's build a pipeline and train the model using sentence embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a89b90e-375e-4caf-9c7a-7b847c45b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = 300\n",
    "\n",
    "def get_sentence_embeddings(sent_df):\n",
    "    new_df = pd.DataFrame(columns=range(EMBEDDINGS))\n",
    "    for i in tqdm(range(len(sent_df))):\n",
    "        text = sent_df.iloc[i]\n",
    "        vec = nlp(text).vector\n",
    "        new_df.loc[len(new_df)] = vec\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce02e4e-f36b-4507-a43f-d0f5108185a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_emb = make_pipeline(\n",
    "    FunctionTransformer(get_sentence_embeddings), \n",
    "    RandomForestClassifier(random_state=613, n_estimators=64, max_depth=16, class_weight=\"balanced\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8104c3ac-eb1d-4b9c-9a30-4a87defc6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16990/16990 [04:41<00:00, 60.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function get_sentence_embeddings at 0x17002de10&gt;)),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=16,\n",
       "                                        n_estimators=64, random_state=613))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function get_sentence_embeddings at 0x17002de10&gt;)),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=16,\n",
       "                                        n_estimators=64, random_state=613))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function get_sentence_embeddings at 0x17002de10&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=16, n_estimators=64,\n",
       "                       random_state=613)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('functiontransformer',\n",
       "                 FunctionTransformer(func=<function get_sentence_embeddings at 0x17002de10>)),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(class_weight='balanced', max_depth=16,\n",
       "                                        n_estimators=64, random_state=613))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_emb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a6e0d1a-9567-4242-ae29-934b30a320cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16990/16990 [04:37<00:00, 61.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9928193054738081"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_emb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c6a21-b0c8-454f-819b-059d1b430ba0",
   "metadata": {},
   "source": [
    "The train accuracy obtained is `0.99`.  Seems we are overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef146fc6-78c3-430c-8f08-8edb06b33d5a",
   "metadata": {},
   "source": [
    "### Results on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b24cf3-0a99-4db7-a5a2-86cd7d506b91",
   "metadata": {},
   "source": [
    "Let's try to fit our results to the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be267a06-307c-4532-ac23-e3f84c607ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4117/4117 [00:47<00:00, 87.29it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe_emb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724039f1-ca57-41d4-8eee-67704d4b857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.18      0.29        73\n",
      "           1       0.79      0.52      0.63       214\n",
      "           2       0.61      0.79      0.69       852\n",
      "           3       1.00      0.34      0.50        77\n",
      "           4       0.87      0.84      0.85        97\n",
      "           5       0.94      0.78      0.85       242\n",
      "           6       0.79      0.47      0.59       146\n",
      "           7       0.89      0.56      0.69       160\n",
      "           8       0.92      0.38      0.53        32\n",
      "           9       0.51      0.80      0.62       336\n",
      "          10       1.00      0.38      0.56        13\n",
      "          11       1.00      0.14      0.25        14\n",
      "          12       0.89      0.43      0.58       119\n",
      "          13       0.89      0.15      0.25       116\n",
      "          14       0.63      0.71      0.67       415\n",
      "          15       0.81      0.55      0.66       125\n",
      "          16       0.74      0.77      0.75       249\n",
      "          17       0.86      0.68      0.76       112\n",
      "          18       0.72      0.83      0.77       528\n",
      "          19       0.59      0.64      0.61       197\n",
      "\n",
      "    accuracy                           0.68      4117\n",
      "   macro avg       0.81      0.55      0.61      4117\n",
      "weighted avg       0.72      0.68      0.67      4117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a19ec2-043c-47c8-8c6e-a828482f992e",
   "metadata": {},
   "source": [
    "## Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff835dec-4d1a-4426-80c9-86aa25f859c3",
   "metadata": {},
   "source": [
    "Overall, analyzing text data for classification problems is not an easy task. It requires further fine-tuning of the models for a (hopefully) better result. Moreover, the data is highly non-linear in nature which suggests that the problem we are facing is more difficult than expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544ddb9-0f3e-4351-b54c-4c810df590c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
